
- **CHAPTER 1. Motivations and Basics**

**[강좌 수강을 환영합니다! 여기부터 꼭 보고 넘어가세요-!](https://www.edwith.org/machinelearning1_17/lecture/40603?isDesc=false)

[1.1. Motivations](https://www.edwith.org/machinelearning1_17/lecture/10574?isDesc=false)

[1.2. MLE](https://www.edwith.org/machinelearning1_17/lecture/10575?isDesc=false)

[1.3. MAP](https://www.edwith.org/machinelearning1_17/lecture/10576?isDesc=false)

[1.4. Probability and Distribution](https://www.edwith.org/machinelearning1_17/lecture/10577?isDesc=false) 


[Ch1. Quiz](https://www.edwith.org/machinelearning1_17/quiz/10578?isDesc=false)**



**CHAPTER 2. Fundamentals of Machine Learning**



**[2.1. Rule Based Machine Learning Overview](https://www.edwith.org/machinelearning1_17/lecture/10579?isDesc=false)

[2.2. Introduction to Rule Based Algorithm](https://www.edwith.org/machinelearning1_17/lecture/10580?isDesc=false)

[2.3. Introduction to Decision Tree](https://www.edwith.org/machinelearning1_17/lecture/10581?isDesc=false)

[2.4. Entropy and Information Gain](https://www.edwith.org/machinelearning1_17/lecture/10582?isDesc=false)**



**[2.5. How to create a decision tree given a training dataset](https://www.edwith.org/machinelearning1_17/lecture/10583?isDesc=false)

[Ch2. Quiz](https://www.edwith.org/machinelearning1_17/quiz/10584?isDesc=false)**



**CHAPTER 3. Naive Bayes Classifier

[3.1. Optimal Classification](https://www.edwith.org/machinelearning1_17/lecture/10585?isDesc=false)

[3.2. Conditional Independence](https://www.edwith.org/machinelearning1_17/lecture/10586?isDesc=false)[3.3. Naive Bayes Classifier](https://www.edwith.org/machinelearning1_17/lecture/10587?isDesc=false)

[3.4. Naive Bayes Classifier Application (Matlab Code)](https://www.edwith.org/machinelearning1_17/lecture/10588?isDesc=false)

[Ch3. Quiz](https://www.edwith.org/machinelearning1_17/quiz/10589?isDesc=false)**



**CHAPTER 4. Logistic Regression

[4.1. Decision Boundary](https://www.edwith.org/machinelearning1_17/lecture/10590?isDesc=false)

[4.2. Introduction to Logistic Regression](https://www.edwith.org/machinelearning1_17/lecture/10591?isDesc=false)

[4.3. Logistic Regression Parameter Approximation 1](https://www.edwith.org/machinelearning1_17/lecture/10592?isDesc=false)

[4.4. Gradient Method](https://www.edwith.org/machinelearning1_17/lecture/10593?isDesc=false)

[4.5. How Gradient method works](https://www.edwith.org/machinelearning1_17/lecture/10594?isDesc=false)

[4.6. Logistic Regression Parameter Approximation 2](https://www.edwith.org/machinelearning1_17/lecture/10595?isDesc=false)

[4.7. Naive Bayes to Logistic Regression](https://www.edwith.org/machinelearning1_17/lecture/10596?isDesc=false)

[4.8. Naive Bayes vs Logistic Regression](https://www.edwith.org/machinelearning1_17/lecture/10597?isDesc=false)

[Ch4. Quiz](https://www.edwith.org/machinelearning1_17/quiz/10598?isDesc=false)**



**CHAPTER 5. Support Vector Machine

[5.1. Decision Boundary with Margin](https://www.edwith.org/machinelearning1_17/lecture/10599?isDesc=false)

[5.2. Maximizing the Margin](https://www.edwith.org/machinelearning1_17/lecture/10600?isDesc=false)

[5.3. SVM with Matlab](https://www.edwith.org/machinelearning1_17/lecture/10601?isDesc=false)

[5.4. Error Handling in SVM](https://www.edwith.org/machinelearning1_17/lecture/10602?isDesc=false)

[5.5. Soft Margin with SVM](https://www.edwith.org/machinelearning1_17/lecture/10603?isDesc=false)

[5.6. Rethinking of SVM](https://www.edwith.org/machinelearning1_17/lecture/10604?isDesc=false)

[5.7. Primal and Dual with KKT Condition](https://www.edwith.org/machinelearning1_17/lecture/10605?isDesc=false)

[5.8. Kernel](https://www.edwith.org/machinelearning1_17/lecture/10606?isDesc=false)

[5.9. SVM with Kernel](https://www.edwith.org/machinelearning1_17/lecture/10607?isDesc=false)

[Ch5. Quiz](https://www.edwith.org/machinelearning1_17/quiz/10608?isDesc=false)**



**CHAPTER 6. Training Testing and Regularization

[6.1. Over-fitting and Under-fitting](https://www.edwith.org/machinelearning1_17/lecture/10609?isDesc=false)

[6.2. Bias and Variance](https://www.edwith.org/machinelearning1_17/lecture/10610?isDesc=false)

[6.3. Occam's Razor](https://www.edwith.org/machinelearning1_17/lecture/10611?isDesc=false)

[6.4. Cross Validation](https://www.edwith.org/machinelearning1_17/lecture/10856?isDesc=false)

[6.5. Performance Metrics](https://www.edwith.org/machinelearning1_17/lecture/10860?isDesc=false)

[6.6. Definition of Regularization](https://www.edwith.org/machinelearning1_17/lecture/10863?isDesc=false)

[6.7. Application of Regularization](https://www.edwith.org/machinelearning1_17/lecture/10866?isDesc=false)

[Ch6. Quiz](https://www.edwith.org/machinelearning1_17/quiz/10889?isDesc=false)**


**CHAPTER 7. Bayesian Network**

**[1 Probability Concepts](https://www.edwith.org/machinelearning2__17/lecture/10573?isDesc=false)

[7.2 Probability Theorems](https://www.edwith.org/machinelearning2__17/lecture/10844?isDesc=false)

[7.3 Interpretation of Bayesian Network](https://www.edwith.org/machinelearning2__17/lecture/10845?isDesc=false)

[7.4 Bayes Ball Algorithm](https://www.edwith.org/machinelearning2__17/lecture/10846?isDesc=false)

[7.5 Factorization of Bayesian networks](https://www.edwith.org/machinelearning2__17/lecture/10847?isDesc=false)

[7.6 Inference Question on Bayesian network](https://www.edwith.org/machinelearning2__17/lecture/10848?isDesc=false)

[7.7 Variable Elimination](https://www.edwith.org/machinelearning2__17/lecture/10849?isDesc=false)

[7.8 Potential Function and Clique Graph](https://www.edwith.org/machinelearning2__17/lecture/10850?isDesc=false)

[7.9 Simple Example of Belief Propagation](https://www.edwith.org/machinelearning2__17/lecture/10851?isDesc=false)

[Chapter 7. Quiz](https://www.edwith.org/machinelearning2__17/quiz/10852?isDesc=false)**


**CHAPTER 8. K-Means Clustering and Gaussian Mixture Model

[8.1 K-Means Algorithm 1](https://www.edwith.org/machinelearning2__17/lecture/10853?isDesc=false)

[8.2 K-Means Algorithm 2](https://www.edwith.org/machinelearning2__17/lecture/10854?isDesc=false)

[8.3 Multinomial Distribution](https://www.edwith.org/machinelearning2__17/lecture/10855?isDesc=false)

[8.4 Multivariate Gaussian Distribution](https://www.edwith.org/machinelearning2__17/lecture/10857?isDesc=false)
[8.5 Gaussian Mixture Model](https://www.edwith.org/machinelearning2__17/lecture/10858?isDesc=false)

[8.6 EM step for Gaussian Mixture Model](https://www.edwith.org/machinelearning2__17/lecture/10859?isDesc=false)

[8.7 Relation between K-means and GMM](https://www.edwith.org/machinelearning2__17/lecture/10861?isDesc=false)

[8.8 Fundamentals of the EM Algorithm](https://www.edwith.org/machinelearning2__17/lecture/10862?isDesc=false)

[8.9 Derivation of EM Algorithm](https://www.edwith.org/machinelearning2__17/lecture/10864?isDesc=false)

[Chapter 8. Quiz](https://www.edwith.org/machinelearning2__17/quiz/10865?isDesc=false)**



**CHAPTER 9. Hidden Markov Model

[9.1 Concept of Hidden Markov Model](https://www.edwith.org/machinelearning2__17/lecture/10868?isDesc=false)

[9.2 Joint and Marginal Probability of HMM](https://www.edwith.org/machinelearning2__17/lecture/10869?isDesc=false)

[9.3 Forward-Backward probability Calculation](https://www.edwith.org/machinelearning2__17/lecture/10870?isDesc=false)

[9.4 Viterbi Decoding Algorithm](https://www.edwith.org/machinelearning2__17/lecture/10871?isDesc=false)

[9.5 Baum-Welch Algorithm](https://www.edwith.org/machinelearning2__17/lecture/10872?isDesc=false)

[Chapter 9. Quiz](https://www.edwith.org/machinelearning2__17/quiz/10873?isDesc=false)**



**CHAPTER 10. Sampling Based Inference

[10.1 Forward Sampling](https://www.edwith.org/machinelearning2__17/lecture/10874?isDesc=false)

[10.2 Rejection Sampling](https://www.edwith.org/machinelearning2__17/lecture/10875?isDesc=false)

[10.3 Importance Sampling](https://www.edwith.org/machinelearning2__17/lecture/10876?isDesc=false)

[10.4 Markov Chain](https://www.edwith.org/machinelearning2__17/lecture/10877?isDesc=false)

[10.5 Markov Chain for Sampling](https://www.edwith.org/machinelearning2__17/lecture/10878?isDesc=false)

[10.6 Metropolis-Hastings Algorithm](https://www.edwith.org/machinelearning2__17/lecture/10879?isDesc=false)

[10.7 Gibbs Sampling](https://www.edwith.org/machinelearning2__17/lecture/10880?isDesc=false)

[10.8 Understand the LDA(Latent Dirichlet Allocation)](https://www.edwith.org/machinelearning2__17/lecture/10881?isDesc=false)

[10.9 Gibbs Sampling for LDA - 1](https://www.edwith.org/machinelearning2__17/lecture/10882?isDesc=false)

[10.10 Gibbs Sampling for LDA -2](https://www.edwith.org/machinelearning2__17/lecture/10883?isDesc=false)

[Chapter 10. Quiz](https://www.edwith.org/machinelearning2__17/quiz/10884?isDesc=false)**

